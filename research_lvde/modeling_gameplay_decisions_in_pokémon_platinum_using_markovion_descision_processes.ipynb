{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Gameplay Decisions in PokÃ©mon Platinum using Markovion Descision Processes\n",
    "\n",
    "TODO: add title page elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT: finish my todo's using factually toned language and restrain from using buzzwords:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "TODO: write an abstract\n",
    "\n",
    "## Inspiration for this study\n",
    "\n",
    "TODO: write that this study was made for my decision theory course. after hearing I would be requirered do to a mix of applied and theoratical research in the context of desision theory, i emediatly thought of a markov chain who could play pokemon. This was inspierd by the following video: https://www.youtube.com/watch?v=jNMWkD5VsZ8, where the creator (Keeyan Ghoreshi) uses the way the game produces random numbers to predict every possible outcome amongst all 4.294.967.296 possible ways the game could be played out, leading to a stream of inputs that will **always** beat the game. This approach requires in depth knowledge about the inner workings of the game. My approach will differ most in this regard. \n",
    "\n",
    "## A more probabalistic approach\n",
    "\n",
    "TODO: write that, for each part of the game, a probabilistic approach will be used to try to beat it. in the video inspiration a mix of statistics and basic probability is used to find outliers in terms of how long a battle can take or the most amount of encounters one could come accross in a given grass patch. my approach will fall more inline with a reinforcement learning styled approach, where I will mostly be utilizing markovian decision processes as find these very fascinating, but if I where to come accross a different approach during my research I will not exclude it from possible things to try.\n",
    "\n",
    "## On model generalization\n",
    "\n",
    "TODO: write that each model will be setup to be the most generic it can be. for example: overworld exploration will be handeled by a single model. if I am unable to create one generalized model for broader task completion within the timeframe I am working in, I will try a more \"devide and conquere\" styled approach, where problems are split up into smaller problems with their own model to solve them.\n",
    "\n",
    "## The goal of this study \n",
    "\n",
    "TODO: write a proper goal chapter, with the primary goal statement being:\n",
    "- Create a probabilistic model that learns to play Pokemon Platinum\n",
    "The minimal viable product will include a model that reaches the tutorial part of how to catch pokemons, however I will strive to create a model that plays trough to at least the first gym battle\n",
    "\n",
    "## Research questions\n",
    "\n",
    "TODO: write the following (sub)-research questions in a proper manner. Start with: \"To compliment the goal of creating a probabelistic model that plays pokemon, i devised the following research questions that should help me to come to a proper solution\"\n",
    "1. what are populair (old and new) reinforcement learning approaches? (library study)\n",
    "   1. pro's and con's comparison\n",
    "   2. assisting methods that can be usefull like statistical methods or others?\n",
    "2. how can markovian decision processes be programmed in python? (available product analysis)\n",
    "3. how can we simulate pokemon platinum in a efficient manner?\n",
    "   1. Efficient meaning low in time/opperation complexity (and perhaps space complexity???)\n",
    "   2. Existing software products (available product analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a Markov Decision Process\n",
    "\n",
    "> A **Markov decision process** is a 4-tuple $(S, A, P_a, R_a)$, where:\n",
    "> \n",
    "> - $( S )$ is a set of states called the **state space**. The state space may be discrete or continuous, like the set of real numbers.\n",
    "> - $( A )$ is a set of actions called the **action space** (alternatively, $A_s$ is the set of actions available from state $s$). As for state, this set may be discrete or > continuous.\n",
    "> - $P_a(s, s')$ is, on an intuitive level, the **probability** that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $t+1$. In general, this probability > transition is defined to satisfy:\n",
    "> \n",
    ">   $$\n",
    ">   \\Pr(s_{t+1} \\in S' \\mid s_t = s, a_t = a) = \\int_{S'} P_a(s, s') \\, ds',\n",
    ">   $$\n",
    "> \n",
    ">   for every $S' \\subseteq S$ measurable. In case the state space is discrete, the integral is intended with respect to the counting measure, simplifying as\n",
    "> \n",
    ">   $$\n",
    ">   P_a(s, s') = \\Pr(s_{t+1} = s' \\mid s_t = s, a_t = a).\n",
    ">   $$\n",
    "> \n",
    ">   If $S \\subseteq \\mathbb{R}^d$, the integral is usually taken with respect to the Lebesgue measure.\n",
    "> \n",
    "> - $R_a(s, s')$ is the **immediate reward** (or expected immediate reward) received after transitioning from state $s$ to state $s'$, due to action $a$.\n",
    "> \n",
    "> A policy function $\\pi$ is a (potentially probabilistic) mapping from state space $( S )$ to action space $( A )$. \n",
    "\n",
    "Reference: [Wikipedia's definition of a Markov Decision Process](https://web.archive.org/web/20241114035948/https://en.wikipedia.org/wiki/Markov_decision_process).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing environments\n",
    "\n",
    "TODO: descsribe the use of images from bulbapedia pared with the interactive map by William Sullivan to create matrixes representing the 'maze' that the agent needs to traverse in a given point in time\n",
    "\n",
    "#### Environments Setup Automization\n",
    "<!-- Environments cell GUID: 7f1582d2-2e3e-4526-b3a1-ff8cace214bb -->\n",
    "\n",
    "#### Battle-ing Environment \n",
    "\n",
    "TODO: describe the standardization of battleing navigation\n",
    "\n",
    "<!-- TODO: write a code block that parcess this file, reads this block, parces the table bellow and for each row: create a markdown cell with the title based on the id and name of the environment + a python cell bellow with some default code -->\n",
    "#### Overworld Locations for Agent Traversal\n",
    "\n",
    "The table bellow...\n",
    "\n",
    "| ID   | Name        | Description |\n",
    "|---   |---          |---          |\n",
    "| 0001 | Player Room | The very first room the player spawns in. Also the room that the player will be relocated to once they used mom as their last source for healing their pokemon |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://pkmnmap4.web.app/\n",
    "\n",
    "https://bulbapedia.bulbagarden.net/wiki/Category:Platinum_locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
