{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Gameplay Decisions in Pokémon Platinum using Markovion Descision Processes\n",
    "\n",
    "TODO: add title page elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "TODO: write an abstract\n",
    "\n",
    "## Inspiration for this Study  \n",
    "\n",
    "This study was undertaken as part of my decision theory course. Upon learning that the course would require a combination of applied and theoretical research within the context of decision theory, I was immediately drawn to the idea of creating a Markov decision process capable of playing *Pokémon Platinum*.  \n",
    "\n",
    "The inspiration for this idea came from a video by Keeyan Ghoreshi ([link](https://www.youtube.com/watch?v=jNMWkD5VsZ8)), in which the creator analyzes how the game generates random numbers to predict every possible outcome among the 4,294,967,296 potential ways the game could unfold. By doing so, Ghoreshi identifies a deterministic sequence of inputs that will always beat the game. This approach, however, relies heavily on an in-depth understanding of the game's internal mechanics.  \n",
    "\n",
    "In contrast, my approach will differ significantly by focusing on building a Markov decision process that does not require extensive reverse engineering of the game’s random number generation. Instead, it will aim to make decisions based on probabilistic modeling of state transitions and outcomes.\n",
    "\n",
    "## A More Probabilistic Approach  \n",
    "\n",
    "For each segment of the game, a probabilistic method will be employed to attempt to progress through it successfully. In the inspiration video, a combination of statistics and basic probability is used to identify outliers, such as the longest possible duration of a battle or the maximum number of encounters in a specific grass patch.  \n",
    "\n",
    "My approach will lean more toward reinforcement learning principles, primarily utilizing Markov decision processes (MDPs). I find MDPs particularly compelling and plan to use them extensively to model and solve decision-making problems within the game. However, my methodology will remain flexible—if I encounter alternative approaches during the course of my research that appear promising, I will consider and potentially incorporate them into my analysis.\n",
    "\n",
    "## On Model Generalization  \n",
    "\n",
    "Each model will be designed to be as general-purpose as possible. For instance, a single model will handle overworld exploration rather than creating separate models for specific areas or tasks. This approach is intended to maximize efficiency and general applicability within the game's framework.  \n",
    "\n",
    "However, if creating a broadly generalized model for broader task completion proves impractical within the given timeframe, I will adopt a \"divide and conquer\" strategy. In this case, larger problems will be divided into smaller, more manageable subproblems, each addressed by its own specialized model. This fallback approach ensures progress remains achievable while maintaining focus on the overall objective.\n",
    "\n",
    "## The Goal of This Study  \n",
    "\n",
    "The primary goal of this study is to **create a probabilistic model that learns to play Pokémon Platinum**. This involves leveraging Markov decision processes or similar probabilistic frameworks to model and solve various challenges encountered in the game.  \n",
    "\n",
    "### Minimal Viable Product (MVP)  \n",
    "The minimum objective is to develop a model capable of learning how to engage in battles and achieve victory. The precise definition of \"winning\" in this context will be refined as the study progresses, incorporating factors such as efficiency, success rate, and adaptability.  \n",
    "\n",
    "### Extended Goals  \n",
    "If time permits, the study will also explore the development of an exploratory model designed to navigate the overworld. This model would aim to traverse the game world efficiently and interact with various game elements, providing a more comprehensive framework for automated gameplay.  \n",
    "\n",
    "By balancing these objectives, the study seeks to demonstrate the feasibility and effectiveness of probabilistic models in addressing the complex decision-making tasks inherent to Pokémon Platinum.\n",
    "\n",
    "## Gradually Increasing the Model's Goal Difficulty  \n",
    "\n",
    "The difficulty of the model's objectives will be increased incrementally as the agent demonstrates successful training and proficiency at simpler tasks. This step-by-step approach ensures that the model builds a solid foundation of skills before tackling more complex challenges.  \n",
    "\n",
    "For example, initial tasks may focus on basic decision-making, such as selecting effective moves during battles or navigating straightforward overworld paths. Once the model achieves consistent success in these areas, the complexity of the tasks will gradually increase to include:  \n",
    "- Optimizing strategies for tougher battles, such as those involving type advantages or multi-stage opponents.  \n",
    "- Navigating more intricate overworld segments with branching paths or hazards.  \n",
    "- Managing resources like healing items or determining when to retreat to a Pokémon Center.  \n",
    "\n",
    "This progressive approach allows the model to adapt and improve over time, ensuring it is equipped to handle increasingly difficult scenarios. Additionally, this structure provides clear milestones for measuring the model’s success throughout its development.  \n",
    "\n",
    "## Research Questions  \n",
    "\n",
    "To complement the goal of creating a probabilistic model that plays Pokémon Platinum, I devised the following research questions to guide me toward a viable solution:  \n",
    "\n",
    "1. **What are popular (both old and new) reinforcement learning approaches?**  \n",
    "   - How do these approaches compare in terms of their strengths and weaknesses?  \n",
    "   - Are there supporting methods, such as statistical techniques or others, that could enhance the effectiveness of reinforcement learning?  \n",
    "\n",
    "2. **How can Markovian decision processes be implemented in Python?**  \n",
    "   - What tools, libraries, or frameworks are available to assist in programming Markovian decision processes?  \n",
    "\n",
    "3. **How can we efficiently simulate Pokémon Platinum?**  \n",
    "   - What does \"efficiency\" entail in this context, particularly regarding time and computational complexity (and potentially space complexity)?  \n",
    "   - What existing software products or tools are available for this purpose, and how can they be utilized or adapted?  \n",
    "\n",
    "These questions aim to provide a structured framework for addressing the theoretical and practical challenges involved in creating a probabilistic model for gameplay. They span foundational research, technical implementation, and performance optimization, ensuring a comprehensive approach to the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing environments\n",
    "\n",
    "TODO: descsribe the use of images from bulbapedia pared with the interactive map by William Sullivan to create matrixes representing the 'maze' that the agent needs to traverse in a given point in time\n",
    "\n",
    "### Environments Setup Automization\n",
    "<!-- Environments cell GUID: 7f1582d2-2e3e-4526-b3a1-ff8cace214bb -->\n",
    "\n",
    "### Battle-ing Environment \n",
    "\n",
    "TODO: describe the standardization of battleing navigation\n",
    "\n",
    "<!-- TODO: write a code block that parcess this file, reads this block, parces the table bellow and for each row: create a markdown cell with the title based on the id and name of the environment + a python cell bellow with some default code -->\n",
    "### Overworld Locations for Agent Traversal\n",
    "\n",
    "The table bellow demonstrates \n",
    "\n",
    "| ID   | Name        | Description |\n",
    "|---   |---          |---          |\n",
    "| 0001 | Player Room | The very first room the player spawns in. Also the room that the player will be relocated to once they used mom as their last source for healing their pokemon |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the battle agent\n",
    "\n",
    "TODO: work out timo's tip bellow in a proper paragraph\n",
    "for an agent to learn a strategy, it needs to be rewarded for a set of sequential moves (temperal difference learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a Markov Decision Process\n",
    "\n",
    "> A **Markov decision process** is a 4-tuple $(S, A, P_a, R_a)$, where:\n",
    "> \n",
    "> - $( S )$ is a set of states called the **state space**. The state space may be discrete or continuous, like the set of real numbers.\n",
    "> - $( A )$ is a set of actions called the **action space** (alternatively, $A_s$ is the set of actions available from state $s$). As for state, this set may be discrete or > continuous.\n",
    "> - $P_a(s, s')$ is, on an intuitive level, the **probability** that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $t+1$. In general, this probability > transition is defined to satisfy:\n",
    "> \n",
    ">   $$\n",
    ">   \\Pr(s_{t+1} \\in S' \\mid s_t = s, a_t = a) = \\int_{S'} P_a(s, s') \\, ds',\n",
    ">   $$\n",
    "> \n",
    ">   for every $S' \\subseteq S$ measurable. In case the state space is discrete, the integral is intended with respect to the counting measure, simplifying as\n",
    "> \n",
    ">   $$\n",
    ">   P_a(s, s') = \\Pr(s_{t+1} = s' \\mid s_t = s, a_t = a).\n",
    ">   $$\n",
    "> \n",
    ">   If $S \\subseteq \\mathbb{R}^d$, the integral is usually taken with respect to the Lebesgue measure.\n",
    "> \n",
    "> - $R_a(s, s')$ is the **immediate reward** (or expected immediate reward) received after transitioning from state $s$ to state $s'$, due to action $a$.\n",
    "> \n",
    "> A policy function $\\pi$ is a (potentially probabilistic) mapping from state space $( S )$ to action space $( A )$. \n",
    "\n",
    "Reference: [Wikipedia's definition of a Markov Decision Process](https://web.archive.org/web/20241114035948/https://en.wikipedia.org/wiki/Markov_decision_process).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://www.youtube.com/watch?v=jNMWkD5VsZ8\n",
    "\n",
    "https://web.archive.org/web/20241114035948/https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "\n",
    "https://pkmnmap4.web.app/\n",
    "\n",
    "https://bulbapedia.bulbagarden.net/wiki/Category:Platinum_locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
