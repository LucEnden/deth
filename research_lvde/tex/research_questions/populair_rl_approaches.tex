\section{What are popular (both old and new) reinforcement learning approaches?  }

In this chapter we will discuss popular reinforcement learning approaches. 
First we will discuss the different aspacts that can be used to categorize reinforcement learning approaches. 
Then we will go into concrete examples of popular reinforcement learning approaches.
After that we will compare these approaches in terms of their strengths and weaknesses usign a \gls{swot} analysis.
Finally, we will discuss if there are supporting methods that could enhance the effectiveness of reinforcement learning.

\subsection{Exploration vs Exploitation}

In reinforcement learning, the \gls{agent} must balance \emph{exploration} and \emph{exploitation} to maximize its cumulative rewards. \emph{Exploration} involves trying new actions to discover their effects on the environment and learn more about the optimal \gls{policy}. \emph{Exploitation}, on the other hand, involves selecting actions that are known to yield high rewards based on the \gls{agent}'s current knowledge. The \gls{agent} must strike a balance between these two strategies to effectively learn and optimize its behavior over time.

\subsection{Value-based vs Policy-based approaches}

\emph{Value-based} approaches focus on learning a \emph{value function}, which estimates the expected cumulative reward of being in a particular state (or state-action pair). The \gls{agent} derives its \gls{policy} indirectly from this value function by selecting actions that lead to states with the highest estimated values. For example, methods like Q-learning aim to iteratively update the value function to converge on the optimal \gls{policy}.

\emph{Policy-based} approaches, in contrast, directly optimize the without explicitly estimating a value function. The \gls{agent} learns a parameterized \gls{policy} that maps states to actions, often using methods like gradient ascent to maximize expected rewards. This approach is particularly well-suited for environments with high-dimensional or continuous action spaces where deriving a \gls{policy} from a value function may be challenging.

\subsection{On-policy vs Off-policy approaches}

\emph{On-\gls{policy}} approaches learn a \gls{policy} while interacting with the environment using the same \gls{policy} that is being evaluated and improved. This means the \gls{agent} uses its current \gls{policy} to generate actions and then updates that \gls{policy} based on the resulting feedback. 

\emph{Off-\gls{policy}} approaches, on the other hand, learn a \gls{policy} while following a different \gls{policy} to generate actions. This allows the \gls{agent} to explore the environment using one \gls{policy} (the behavior \gls{policy}) while improving another \gls{policy} (the target \gls{policy}).

\subsection{Model-based vs Model-free approaches}

\emph{Model-based} approaches rely on a \gls{transition model} of the environment. This model describes how actions taken by the \gls{agent} lead to different states and rewards. If the model is unknown, the \gls{agent} can learn it by observing the effects of its actions. With this model, the \gls{agent} can plan its behavior by simulating potential outcomes and selecting actions that optimize future rewards.

\emph{Model-free} approaches, on the other hand, bypass the need for a \gls{transition model} entirely. Instead of explicitly learning how the environment transitions between states, the \gls{agent} focuses on directly learning the optimal \gls{policy} or value function that maps states to actions based on observed rewards. This eliminates the need to model the environment but requires a more direct trial-and-error approach to improve behavior.

\subsection{Action-Cost function and Reward Function}

\emph{\gls{action-cost function}} is a function $c(s, a, s')$ that assigns a cost to taking action $a$ in state $s$ to eventually reach state $s'$.
\emph{\gls{reward function}s} $R(s, a, s')$ give a reward to the \gls{agent} for taking action $a$ in state $s$ and transitioning to state $s'$. The goal of the \gls{agent} is to maximize the expected sum of rewards over time by selecting actions that minimize costs and maximize rewards.

\newpage
\subsection{To summarize}

Before we start the comparative analysis of popular reinforcement learning approaches, let's summarize the above.

Summery of the subsections:
\begin{itemize}
    \item exploration involves trying new actions to learn more about the environment, while exploitation involves selecting actions that are known to yield high rewards.
    \item value-based methods aim to determine optimal actions by learning value functions, while \gls{policy}-based methods focus on learning the optimal \gls{policy} directly.
    \item on-\gls{policy} methods evaluate and improve the \gls{policy} being used, while off-\gls{policy} methods evaluate one \gls{policy} while potentially following another. See: \cite{rn2022aima}, page 853-854.
    \item if an approach uses a model of the environment to plan its actions, it is model-based. If it does not use a model, it is model-free. See: \cite{rn2022aima}, page 841.
    \item the \gls{reward function} $R(s, a, s')$ gives a reward to the \gls{agent} for taking action $a$ in state $s$ and transitioning to state $s'$. The \gls{action-cost function} $c(s, a, s')$ assigns a cost to taking action $a$ in state $s$ to reach state $s'$. See: \cite{rn2022aima}, pages 553 and 83 respectively.
\end{itemize}

In reinforcement learning, an agent interacts with an environment to learn how to behave optimally.
An agent performs actions conform to a \gls{policy} $pi$ to maximize the expected sum of rewards. 
The \gls{policy} dictates which action to take in a given state $s$, with the end goal of reaching some desired state $s'$.
What each action does is described by the \gls{transition model}.
An action cost function $c(s, a, s')$ assigns a cost to taking an action and if that action results in reaching $s'$, a \gls{reward function} $R(s, a, s')$ assigns a reward to taking an action and reaching $s'$.

\newpage
\subsection{Comparative Analysis on Popular RL Approaches}

This section provides a brief description of several popular reinforcement learning approaches, their core principles, and how they differ in solving decision-making problems.
\begin{itemize}
    \item Q-learning (\cite{rn2022aima}, page 841): Q-learning is a model-free, off-policy RL algorithm used to learn the optimal action-value function, Q(s,a)Q(s,a), which estimates the cumulative reward for taking an action aa in a state ss and following the optimal policy thereafter. It updates Q-values iteratively using the Bellman equation and does not require a model of the environment.
    \item SARSA (\cite{rn2022aima}, page 853): SARSA (State-Action-Reward-State-Action) is a model-free, on-policy RL algorithm. Unlike Q-learning, it updates the Q-value based on the action actually taken under the current policy. This makes it sensitive to the exploration strategy, as it learns the Q-values for the policy being followed.
    \item Deep Q-Network (DQN) (\cite{rn2022aima}, page 867): DQN combines Q-learning with deep learning by using a neural network to approximate the Q-function. It incorporates techniques like experience replay and target networks to stabilize training and enable RL to scale to problems with high-dimensional state spaces.
    \item Proximal Policy Optimization (PPO) (\cite{rn2022aima}, page 872): PPO is a policy-gradient-based algorithm that seeks to optimize a stochastic policy by using a clipped objective function to balance exploration and exploitation. It simplifies trust-region optimization, making it more computationally efficient while maintaining stable learning.
    \item Deep Deterministic Policy Gradient (DDPG): DDPG is an actor-critic algorithm designed for continuous action spaces. It combines deterministic policy gradients with deep neural networks, enabling the agent to learn both a policy (actor) and a value function (critic). It leverages experience replay and target networks to improve stability.
    \item Trust Region Policy Optimization (TRPO): TRPO is a policy optimization method that enforces a trust region constraint to ensure stable policy updates. It uses a surrogate loss function to improve the policy iteratively while limiting the magnitude of each update to maintain training stability.
    \item Soft Actor-Critic (SAC): SAC is an off-policy, actor-critic algorithm that optimizes a stochastic policy while incorporating an entropy term in the objective function. The entropy term encourages exploration by maximizing the randomness of the policy, leading to more robust learning in complex environments.
\end{itemize}

\subsection{How do these approaches compare in terms of their strengths and weaknesses?  }

Unfortunately, due to time constraints, the SWOT analysis won't be included in this document.

\subsection{Are there supporting methods, such as statistical techniques or others, that could enhance the effectiveness of reinforcement learning?}

Similair to the SWOT analysis, the discussion on supporting methods won't be included in this document due to time constraints.
