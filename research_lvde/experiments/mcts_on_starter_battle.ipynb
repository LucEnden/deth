{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search on Starter Battle\n",
    "\n",
    "In this experiment, I will be exploring the use of Monte Carlo Tree Search (MCTS) on the Starter Battle environment. The goal is to see how well MCTS can perform in this environment and how it compares to the DQN model from the [initial_pokemon_battleing_agent](./initial_pokemon_battleing_agent.ipynb) experiment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure relative imports work correctly\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Space\n",
    "\n",
    "from services.starter_pokemons import starter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model based approach\n",
    "\n",
    "The DQN model from the earlier experiment would be considered a model free approach to solving the starter battle. For my learning outcomes however, I am obligated to explore a model based approach. MCTS is my approach of choice. My plan is to implement the transition model as a tree and use MCTS to search through the tree to find the best move. I was inspired to use MCTS by google's AlphaGo which uses MCTS to search through the game tree to find the best move.\n",
    "\n",
    "### How MCTS works\n",
    "![Image from wikipedia about how MCTS works](https://upload.wikimedia.org/wikipedia/commons/a/a6/MCTS_Algorithm.png)\n",
    "*Image from wikipedia about how MCTS works*\n",
    "\n",
    "### How a tree representing pokemon states differers from a traditional game tree\n",
    "\n",
    "TODO: describe the fact that a tree representing pokemon game does not have a set root node, which is different from a traditional game tree like one for chess (which always has the same starting state)\n",
    "\n",
    "### MCTS requires a perfect information game\n",
    "\n",
    "TODO: describe that MCTS requires a perfect information game, which essentially is the case for pokemon. However, I stated before that in this research I would like to treat the game as if its not. So I am breaking my own rules at this point, but I am willing to do so for learning purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration for tree implementation\n",
    "\n",
    "https://gist.github.com/qpwo/c538c6f73727e254fdc7fab81024f6e1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_space = gym.spaces.Discrete(starter_df['hp'].max() + 1)\n",
    "attack_space = gym.spaces.Discrete(starter_df['attack'].max() + 1)\n",
    "defense_space = gym.spaces.Discrete(starter_df['defense'].max() + 1)\n",
    "# sp_atk_space = gym.spaces.Discrete(starter_df['sp. atk'].max() + 1)\n",
    "# sp_def_space = gym.spaces.Discrete(starter_df['sp. def'].max() + 1)\n",
    "speed_space = gym.spaces.Discrete(starter_df['speed'].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_stage_space = gym.spaces.Box(low=0, high=12, shape=(6,), dtype=np.int8)\n",
    "def map_stat_stages(stat_stages: list[int]) -> np.ndarray:\n",
    "    if len(stat_stages) != 6:\n",
    "        raise ValueError('Expected exactly 6 stat stages')\n",
    "    \n",
    "    # map from -6 / 6 to 0 / 12\n",
    "    return np.array(stat_stages) + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poke_battle_sim as pb\n",
    "\n",
    "action_mappings = {\n",
    "    0: ('move', 0, 0),\n",
    "    1: ('move', 0, 1),\n",
    "}\n",
    "action_space = gym.spaces.Discrete(len(action_mappings))\n",
    "\n",
    "def get_action(action: int, trainer: pb.Trainer) -> tuple[str, int]:\n",
    "    action = action_mappings[action][0]\n",
    "    return [\n",
    "        action[0], # The actual action\n",
    "        trainer.poke_list[action[1]].moves[action[2]].name\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Implementation\n",
    "\n",
    "Bellow is a class implementing the tree I will be using for the MCTS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A minimal implementation of Monte Carlo tree search (MCTS) in Python 3\n",
    "Luke Harold Miles, July 2019, Public Domain Dedication\n",
    "See also https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\n",
    "https://gist.github.com/qpwo/c538c6f73727e254fdc7fab81024f6e1\n",
    "\"\"\"\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"Monte Carlo tree searcher. First rollout the tree then choose a move.\"\n",
    "\n",
    "    def __init__(self, exploration_weight=1):\n",
    "        self.Q = defaultdict(int)  # total reward of each node\n",
    "        self.N = defaultdict(int)  # total visit count for each node\n",
    "        self.children = dict()  # children of each node\n",
    "        self.exploration_weight = exploration_weight\n",
    "\n",
    "    def choose(self, node):\n",
    "        \"Choose the best successor of node. (Choose a move in the game)\"\n",
    "        if node.is_terminal():\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "\n",
    "        if node not in self.children:\n",
    "            return node.find_random_child()\n",
    "\n",
    "        def score(n):\n",
    "            if self.N[n] == 0:\n",
    "                return float(\"-inf\")  # avoid unseen moves\n",
    "            return self.Q[n] / self.N[n]  # average reward\n",
    "\n",
    "        return max(self.children[node], key=score)\n",
    "\n",
    "    def do_rollout(self, node):\n",
    "        \"Make the tree one layer better. (Train for one iteration.)\"\n",
    "        path = self._select(node)\n",
    "        leaf = path[-1]\n",
    "        self._expand(leaf)\n",
    "        reward = self._simulate(leaf)\n",
    "        self._backpropagate(path, reward)\n",
    "\n",
    "    def _select(self, node):\n",
    "        \"Find an unexplored descendent of `node`\"\n",
    "        path = []\n",
    "        while True:\n",
    "            path.append(node)\n",
    "            if node not in self.children or not self.children[node]:\n",
    "                # node is either unexplored or terminal\n",
    "                return path\n",
    "            unexplored = self.children[node] - self.children.keys()\n",
    "            if unexplored:\n",
    "                n = unexplored.pop()\n",
    "                path.append(n)\n",
    "                return path\n",
    "            node = self._uct_select(node)  # descend a layer deeper\n",
    "\n",
    "    def _expand(self, node):\n",
    "        \"Update the `children` dict with the children of `node`\"\n",
    "        if node in self.children:\n",
    "            return  # already expanded\n",
    "        self.children[node] = node.find_children()\n",
    "\n",
    "    def _simulate(self, node):\n",
    "        \"Returns the reward for a random simulation (to completion) of `node`\"\n",
    "        invert_reward = True\n",
    "        while True:\n",
    "            if node.is_terminal():\n",
    "                reward = node.reward()\n",
    "                return 1 - reward if invert_reward else reward\n",
    "            node = node.find_random_child()\n",
    "            invert_reward = not invert_reward\n",
    "\n",
    "    def _backpropagate(self, path, reward):\n",
    "        \"Send the reward back up to the ancestors of the leaf\"\n",
    "        for node in reversed(path):\n",
    "            self.N[node] += 1\n",
    "            self.Q[node] += reward\n",
    "            reward = 1 - reward  # 1 for me is 0 for my enemy, and vice versa\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "        \"Select a child of node, balancing exploration & exploitation\"\n",
    "\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "\n",
    "        log_N_vertex = math.log(self.N[node])\n",
    "\n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            return self.Q[n] / self.N[n] + self.exploration_weight * math.sqrt(\n",
    "                log_N_vertex / self.N[n]\n",
    "            )\n",
    "\n",
    "        return max(self.children[node], key=uct)\n",
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    \"\"\"\n",
    "    A representation of a single board state.\n",
    "    MCTS works by constructing a tree of these Nodes.\n",
    "    Could be e.g. a chess or checkers board state.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_children(self):\n",
    "        \"All possible successors of this board state\"\n",
    "        return set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_random_child(self):\n",
    "        \"Random successor of this board state (for more efficient simulation)\"\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self):\n",
    "        \"Returns True if the node has no children\"\n",
    "        return True\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward(self):\n",
    "        \"Assumes `self` is terminal node. 1=win, 0=loss, .5=tie, etc\"\n",
    "        return 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def __hash__(self):\n",
    "        \"Nodes must be hashable\"\n",
    "        return 123456789\n",
    "\n",
    "    @abstractmethod\n",
    "    def __eq__(node1, node2):\n",
    "        \"Nodes must be comparable\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree starts with a special game node.\n",
    "- In board games, the starting state is usually constant. This is not the case for pokemon battles.\n",
    "- Thus our root node will just be a special node\n",
    "- Its children will be composed of all possible starting states, once we populate the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootNode(Node):\n",
    "    def __init__(self):\n",
    "        self.children = set()\n",
    "\n",
    "    def find_children(self):\n",
    "        return self.children\n",
    "    \n",
    "    def find_random_child(self):\n",
    "        return random.choice(list(self.children))\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return False\n",
    "    \n",
    "    def reward(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree will follow a similair structure as seen in the image above\n",
    "- Each vertex will represent a state of the game\n",
    "- Their edges will represent the actions possible from the given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerState():\n",
    "    def __init__(self, hp: int, attack: int, defense: int, speed: int, stat_stages):\n",
    "        self.hp = hp\n",
    "        self.attack = attack\n",
    "        self.defense = defense\n",
    "        self.speed = speed\n",
    "        self.stat_stages = stat_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateNode(Node):\n",
    "    def __init__(self, agent: TrainerState, opponent: TrainerState):\n",
    "        self.agent = agent\n",
    "        self.opponent = opponent\n",
    "        self.children = set()\n",
    "    \n",
    "    def find_children(self):\n",
    "        return self.children\n",
    "    \n",
    "    def find_random_child(self):\n",
    "        return random.choice(list(self.children))\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return self.agent.hp <= 0 or self.opponent.hp <= 0\n",
    "    \n",
    "    def reward(self):\n",
    "        # TODO find a more complex way to get rewards, as this will just result in finding the quickest way to win\n",
    "        # The Q values arising from this wont be very interasting\n",
    "        if self.agent.hp <= 0:\n",
    "            return -1\n",
    "        elif self.opponent.hp <= 0:\n",
    "            return 1\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.agent, self.opponent))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.agent == other.agent and self.opponent == other.opponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It seems to me that building a transition model (i.e. using a model based approach) is most viable when it is easy (i.e. wont take to long) to exhaustively turn all the rules and actions of a problem into code. For example, chess has pretty simple and relativly small set of rules. Their are only so many moves a piece can make, and their are not that many pieces. They have very predictable behavior, which makes it easy to implement a transition model.\n",
    "\n",
    "Pokemon on the other hand, has 493 pieces and 215 unique move effects. Not to mention the fact a trainer can have items to its disposale. This makes it very hard to implement a complete transition model. Utilizing other peoples work (like for example, using the effect methods from `poke_battle_sim.util.process_move`) could make implementing a complete transition model easier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
